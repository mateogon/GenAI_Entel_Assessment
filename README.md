# ü§ñ Assessment AI Engineer - An√°lisis Sem√°ntico de Transcripciones (Entel GenAI)

Este repositorio contiene la soluci√≥n para el assessment de AI Engineer del equipo GenAI de Entel. El proyecto implementa un sistema backend escalable para analizar sem√°nticamente transcripciones de llamadas de atenci√≥n al cliente, utilizando la API de OpenAI de forma optimizada y la base de datos vectorial Qdrant.

## üìù Descripci√≥n General

El sistema permite:

- **üîç B√∫squeda:** Encontrar transcripciones relevantes mediante palabras clave o significado sem√°ntico, consultando eficientemente la base de datos vectorial Qdrant.
- **üìä Extracci√≥n de Temas:** Identificar los temas principales discutidos en una conversaci√≥n, usando llamadas as√≠ncronas a OpenAI para no bloquear la API.
- **üè∑Ô∏è Clasificaci√≥n Autom√°tica:** Asignar categor√≠as predefinidas (ej: "Problemas T√©cnicos", "Soporte Comercial") a las llamadas, mediante llamadas as√≠ncronas a OpenAI.

Se ha desarrollado priorizando la escalabilidad (uso de Qdrant y asyncio), eficiencia (procesamiento batch de embeddings) y una gesti√≥n cuidadosa del presupuesto de OpenAI ($5 USD).

## ‚ú® Caracter√≠sticas Principales

- **Backend API:** Construido con FastAPI, proporcionando endpoints RESTful para b√∫squeda y an√°lisis. Operaciones as√≠ncronas para un an√°lisis no bloqueante.
- **Base de Datos Vectorial:** Utiliza Qdrant (ejecutado v√≠a Docker) para almacenar embeddings y texto, permitiendo b√∫squedas sem√°nticas y por palabra clave r√°pidas y escalables.
- **Procesamiento NLP:**
  - Se utiliza `text-embedding-3-small` de OpenAI para b√∫squeda sem√°ntica. Los embeddings se generan en lotes y se almacenan en Qdrant.
  - Se emplea `gpt-4o-mini` de OpenAI para la extracci√≥n de temas y clasificaci√≥n mediante llamadas as√≠ncronas.
- **Optimizaci√≥n de Costos:**
  - Generaci√≥n batch de embeddings para minimizar las llamadas a la API.
  - Uso de modelos eficientes (`text-embedding-3-small` y `gpt-4o-mini`).
  - Modo de simulaci√≥n controlable (configurable v√≠a `ENABLE_OPENAI_CALLS` en el archivo `.env`) para desarrollo y pruebas sin consumir cr√©ditos de OpenAI.
- **Preprocesamiento:** Limpieza de texto, manejo de transcripciones en formato `.txt` y anonimizaci√≥n de PII (incluyendo RUT chileno) con Presidio.
- **Interfaz:** Frontend b√°sico desarrollado con Streamlit para interactuar con la API.

## Interfaz

A continuaci√≥n se muestra una vista previa del frontend:

![Frontend Demo](assets/demo.png)


## ‚öôÔ∏è Configuraci√≥n del Entorno

Sigue estos pasos para configurar el proyecto localmente (probado en Windows con PowerShell, adaptable a Linux/Mac):

1. **Instalar Docker**  
   Aseg√∫rate de tener Docker Desktop instalado y en ejecuci√≥n. Es necesario para Qdrant.

2. **Clonar el Repositorio**
   ```bash
   git clone https://github.com/mateogon/GenAI_Entel_Assessment.git
   cd GenAI_Entel_Assessment
   ```

3. **Crear y Activar el Entorno Virtual**
   ```bash
   python -m venv .venv
   # En PowerShell, podr√≠as necesitar ajustar la pol√≠tica de ejecuci√≥n:
   # Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
   .venv\Scripts\Activate.ps1
   ```
   *(En Linux/Mac: `source .venv/bin/activate`)*

4. **Instalar Dependencias**
   ```bash
   pip install -r requirements.txt
   ```
   Esto instalar√° FastAPI, Uvicorn, Streamlit, OpenAI, Qdrant client, Presidio, spaCy, etc.

5. **Descargar el Modelo de Lenguaje spaCy** *(necesario para Presidio)*
   ```bash
   python -m spacy download es_core_news_md
   ```

6. **Configurar la API Key de OpenAI**
   - Crea un archivo llamado `.env` en la ra√≠z del proyecto.
   - A√±ade tu clave de API:
     ```
     OPENAI_API_KEY=sk-TuClaveRealDeOpenAIaqui
     ```

7. **Control del Modo de Simulaci√≥n**
   - Para deshabilitar las llamadas reales a GPT (modo simulaci√≥n, recomendado para pruebas):
     ```
     ENABLE_OPENAI_CALLS=false
     ```
   - Para habilitar las llamadas reales (consumir√° cr√©ditos):
     ```
     ENABLE_OPENAI_CALLS=true
     ```
   *Si la l√≠nea `ENABLE_OPENAI_CALLS` no existe, se operar√° en modo simulaci√≥n (false) por defecto.*

8. **Colocar Datos Crudos**  
   Aseg√∫rate de que las 100 transcripciones en formato `.txt` (ej: `sample_01.txt`) est√©n dentro de la carpeta `data/raw/`.

## üöÄ Ejecuci√≥n

### Iniciar Qdrant (Base de Datos Vectorial)
Abre una terminal y ejecuta:
```bash
docker run -d --name qdrant_db -p 6333:6333 qdrant/qdrant
```
El puerto `6333` es para la API y para la UI web ([http://localhost:6333/dashboard](http://localhost:6333/dashboard)).

### Preparar Datos y Generar Embeddings  
*(Ejecutar solo una vez inicialmente o si los datos cambian)*

1. **Preprocesamiento (Limpieza y Anonimizaci√≥n)**
   ```bash
   python scripts/preprocess_data.py
   ```
   Este script lee los archivos `.txt` de `data/raw/`, los limpia/anonimiza y guarda archivos `.json` en `data/processed/`.

2. **Generaci√≥n de Embeddings e Indexaci√≥n en Qdrant**  
   *(Aseg√∫rate de que `ENABLE_OPENAI_CALLS=true` en `.env` si es la primera vez o si necesitas regenerar embeddings)*
   ```bash
   python scripts/generate_embeddings_openai.py
   ```
   Este script lee los archivos JSON procesados, genera embeddings en lotes usando la API de OpenAI y los inserta en Qdrant. Si la colecci√≥n ya existe, te preguntar√° si deseas recrearla (lo cual borrar√° los datos actuales).

### Iniciar el Backend (API FastAPI)

- **Para desarrollo simple (Windows/Linux/Mac)**
  ```bash
  uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
  ```

- **Alternativa para Windows (simulando un servidor de producci√≥n robusto)**
  ```bash
  # Instala waitress si a√∫n no lo hiciste: pip install waitress
  waitress-serve --host 0.0.0.0 --port 8000 app.main:app
  ```

- **Para producci√≥n en Linux/Docker (recomendado)**
  ```bash
  # Instala gunicorn: pip install gunicorn
  gunicorn -w 4 -k uvicorn.workers.UvicornWorker app.main:app --bind 0.0.0.0:8000
  ```
  *(Ajusta el n√∫mero de workers (`-w 4`) seg√∫n tus recursos.)*

La API estar√° disponible en [http://localhost:8000](http://localhost:8000).  
Accede a la documentaci√≥n interactiva en:
- **Swagger UI:** [http://localhost:8000/docs](http://localhost:8000/docs)
- **ReDoc:** [http://localhost:8000/redoc](http://localhost:8000/redoc)

### Iniciar el Frontend (Streamlit - Opcional)

Abre otra terminal en la ra√≠z del proyecto, activa el entorno virtual y ejecuta:
```bash
streamlit run frontend.py
```
Se abrir√° autom√°ticamente una pesta√±a en tu navegador (usualmente en [http://localhost:8501](http://localhost:8501)).

## ‚öôÔ∏è Arquitectura y Decisiones T√©cnicas

- **Backend:** FastAPI por su rendimiento, soporte nativo para `asyncio` y auto-documentaci√≥n.
- **Procesamiento de Texto Crudo:** Scripts en Python que utilizan expresiones regulares y Presidio para el parseo y la anonimizaci√≥n.
- **Almacenamiento y B√∫squeda:** Qdrant se emplea como base de datos vectorial. Almacena los embeddings y el texto completo anonimizado, permitiendo b√∫squedas sem√°nticas y por palabra clave sin necesidad de cargar todos los datos en RAM.
- **An√°lisis (Temas/Clasificaci√≥n):** Uso de `gpt-4o-mini` de OpenAI a trav√©s de AsyncOpenAI para extraer temas y clasificar transcripciones.
- **Generaci√≥n de Embeddings:** Optimizada mediante llamadas batch a la API de OpenAI (`get_embeddings_batch`), reduciendo la latencia y la cantidad de llamadas.
- **Gesti√≥n de Presupuesto:** Uso de modelos eficientes, batching y modo de simulaci√≥n (`ENABLE_OPENAI_CALLS`) para controlar el gasto.
- **Escalabilidad:**
  - **Datos:** Qdrant permite escalar el almacenamiento de datos vectoriales y textuales m√°s all√° de la memoria RAM.
  - **Concurrencia en la API:** `asyncio` en FastAPI y llamadas as√≠ncronas a OpenAI facilitan el manejo de m√∫ltiples peticiones simult√°neas.
  - **Ingesta:** El procesamiento batch acelera la carga inicial y las actualizaciones masivas.
- **Despliegue:** Se recomienda el uso de Gunicorn (Linux/Docker) o Waitress con m√∫ltiples instancias detr√°s de un balanceador de carga para aprovechar m√∫ltiples n√∫cleos o m√°quinas.
- **Frontend:** Streamlit permite crear prototipos de UI de forma r√°pida y sencilla.

## üìñ Documentaci√≥n de la API

La API backend, construida con FastAPI, expone endpoints RESTful para realizar b√∫squedas y an√°lisis sobre las transcripciones.

*   **Documentaci√≥n Interactiva (Auto-generada):** La forma m√°s completa y actualizada para explorar los endpoints, ver los esquemas de datos exactos y probar la API directamente desde el navegador est√° disponible cuando el backend est√° en ejecuci√≥n:
    *   Swagger UI: [http://localhost:8000/docs](http://localhost:8000/docs)
    *   ReDoc: [http://localhost:8000/redoc](http://localhost:8000/redoc)

*   **Documentaci√≥n Detallada y Ejemplos:** Para una descripci√≥n m√°s detallada de cada endpoint, explicaciones de casos de uso, ejemplos pr√°cticos con `curl`, y ejemplos de respuestas JSON, por favor consulta el documento dedicado:
    *   **‚û°Ô∏è [Ver Documentaci√≥n Detallada de la API (API_DOCUMENTATION.md)](API_DOCUMENTATION.md)**